
Directory: .
============

File: .\agents_ls.py
--------------------
import json
import random
import uuid

import agent_ls
import model_manager
from agent_ls import AgentType, AGENT_CREATORS, AGENTS_USING_NAMES

config_file_name = "model_library.json"


class AgentManagerLS:
    '''
    Use AgentType Enum to see the agent types available.
    If a model is not specified, a random (loaded) model will be choosen.
    '''
    def __init__(self):
        self.model_configs = self.load_model_configs(config_file_name)
        self.loaded_models = model_manager.load_and_save_models()
        self.matched_models = self.match_models_with_configs(self.loaded_models, self.model_configs)
        
        # Default code execution config
        self.default_code_execution_config = {"use_docker": False}
        
        
    @staticmethod
    def load_model_configs(filename):
        with open(filename, 'r') as f:
            return json.load(f)


    @staticmethod
    def match_models_with_configs(loaded_models, model_configs):
        matched_models = {}
        for model in loaded_models:
            short_name = model.rsplit('/', 1)[-1]
            if short_name in model_configs:
                matched_models[short_name] = model_configs[short_name]
        return matched_models


    def get_available_models(self) -> str:
        return list(self.matched_models.keys())
    
    
    def print_available_models(self):
        print("\nAvailable models:")
        for model_name in self.get_available_models():
            print(f"- {model_name}")


    def create(self, agent_type: AgentType, name = "", model_name = "", **kwargs):
        
        #print("agent_type", agent_type, "AGENT_CREATORS", AGENT_CREATORS)
        # group_chat = agent_manager.create("Chat Group", AgentType.CHAT_GROUP)
        if agent_type not in AGENT_CREATORS:
            
            raise ValueError(f"Unknown agent type: {agent_type}")
        
        if not model_name:
            available_models = self.get_available_models()
            
            if available_models:
                model_name = random.choice(available_models)
                print(f"Agent {name} {agent_type} was assigned a random model: {model_name}")
        
        else:
            print(f"Agent {name} {agent_type} was assigned the model: {model_name}")
                
        if model_name and model_name not in self.matched_models:
            self.print_available_models()
            raise ValueError(f"Model {model_name} not found in available models.")

        model_config = self.matched_models[model_name]
        config_list = model_config['config_list']
        llm_config = {
            "config_list": config_list,
            "cache_seed": model_config['cache_seed'],
            "temperature": config_list[0]['temperature'],
            "max_tokens": model_config['max_tokens']
        }
        
        
        
        if agent_type in AGENTS_USING_NAMES:
            if not name:
                name = f"{model_name}_{uuid.uuid4().hex[:8]}"
            kwargs.update({"name": name})
            
        kwargs.update({"llm_config": llm_config})
        
        # Ensure agents is provided for GroupChat
        if agent_type == AgentType.GROUP_CHAT:
            if "agents" not in kwargs or kwargs["agents"] is None:
                raise ValueError("GroupChat requires a non-empty list of agents in the 'agents' parameter.")

        return AGENT_CREATORS[agent_type](kwargs)


# Usage example
if __name__ == "__main__":
    agent_ls = AgentManagerLS()
    
    print("Available agent types:")
    for agent_type in AgentType:
        print(f"- {agent_type.name}")
    
    available_models = agent_ls.get_available_models()
    model = random.choice(available_models)
    
    if available_models:
        model = agent_ls.user_specified_model
        if not model:
            model = random.choice(available_models)
            
        conversable_agent = agent_ls.create(AgentType.CONVERSABLE_AGENT)
        
        # Create an assistant agent with the second available model (if there is one)
        if len(available_models) > 1:
            assistant_agent = agent_ls.create(AgentType.ASSISTANT_AGENT, available_models[1])
            print(f"Created an Assistant Agent: {assistant_agent.name}")
    else:
        print("No models available. Please check your model configurations.")



File: .\agent_factory.py
------------------------
import autogen
from enum import Enum
import uuid

class AgentType(Enum):
    CONVERSABLE_AGENT = "conversable_agent"
    ASSISTANT_AGENT = "assistant_agent"
    USER_PROXY_AGENT = "user_proxy_agent"
    RETRIEVE_USER_PROXY_AGENT = "retrieve_user_proxy_agent"
    GROUP_CHAT_MANAGER = "group_chat_manager"
    TEACHABLE_AGENT = "teachable_agent"
    AUTOGEN_CODER = "autogen_coder"
    CONFIGURABLE_AGENT = "configurable_agent"
    HUMAN_PROXY_AGENT = "human_proxy_agent"
    GROUP_CHAT = "chat_group"

AGENTS_USING_NAMES = [
    AgentType.CONVERSABLE_AGENT,
    AgentType.ASSISTANT_AGENT,
    AgentType.USER_PROXY_AGENT,
    AgentType.TEACHABLE_AGENT,
    AgentType.AUTOGEN_CODER,
    AgentType.CONFIGURABLE_AGENT,
    AgentType.HUMAN_PROXY_AGENT,
]

default_code_execution_config = {"use_docker": True}

AGENT_CREATORS = {
    AgentType.CONVERSABLE_AGENT: lambda params: autogen.ConversableAgent(**params),
    AgentType.ASSISTANT_AGENT: lambda params: autogen.AssistantAgent(**params),
    AgentType.USER_PROXY_AGENT: lambda params: autogen.UserProxyAgent(**params),
    AgentType.RETRIEVE_USER_PROXY_AGENT: lambda params: autogen.RetrieveUserProxyAgent(**params),
    AgentType.TEACHABLE_AGENT: lambda params: autogen.TeachableAgent(**params),
    AgentType.AUTOGEN_CODER: lambda params: autogen.AutoGenCoder(**params),
    AgentType.CONFIGURABLE_AGENT: lambda params: autogen.ConfigurableAgent(**params),
    AgentType.HUMAN_PROXY_AGENT: lambda params: autogen.HumanProxyAgent(**params),
    AgentType.GROUP_CHAT: lambda params: autogen.GroupChat(**params),
    AgentType.GROUP_CHAT_MANAGER: lambda params: autogen.GroupChatManager(**params),
}

def create_agent(agent_type, name="", model_config=None, obj=None, **kwargs):
    if agent_type not in AGENT_CREATORS:
        raise ValueError(f"Unknown agent type: {agent_type}")

    agent_params = {
        "name": name or f"Agent_{uuid.uuid4().hex[:8]}",
        "llm_config": model_config,
        "code_execution_config": default_code_execution_config,
    }
    
    if agent_type in AGENTS_USING_NAMES:
        agent_params["system_message"] = f"You are an AI assistant named {agent_params['name']}."
    
    agent_params.update(kwargs)
    
    if agent_type == AgentType.GROUP_CHAT:
        return AGENT_CREATORS[agent_type](obj)
    elif agent_type == AgentType.GROUP_CHAT_MANAGER:
        return AGENT_CREATORS[agent_type](groupchat=obj)
    else:
        return AGENT_CREATORS[agent_type](agent_params)


File: .\agent_ls.py
-------------------
from enum import Enum
import autogen

class AgentType(Enum):
    CONVERSABLE_AGENT = "conversable_agent"
    ASSISTANT_AGENT = "assistant_agent"
    USER_PROXY_AGENT = "user_proxy_agent"
    RETRIEVE_USER_PROXY_AGENT = "retrieve_user_proxy_agent"
    GROUP_CHAT_MANAGER = "group_chat_manager"
    TEACHABLE_AGENT = "teachable_agent"
    AUTOGEN_CODER = "autogen_coder"
    CONFIGURABLE_AGENT = "configurable_agent"
    HUMAN_PROXY_AGENT = "human_proxy_agent"
    GROUP_CHAT = "chat_group"
    
AGENTS_USING_NAMES = [
    AgentType.CONVERSABLE_AGENT,
    AgentType.ASSISTANT_AGENT,
    AgentType.USER_PROXY_AGENT,
    AgentType.TEACHABLE_AGENT,
    AgentType.AUTOGEN_CODER,
    AgentType.CONFIGURABLE_AGENT,
    AgentType.HUMAN_PROXY_AGENT,
]

default_code_execution_config = {"use_docker": True}
    
AGENT_CREATORS = {
    AgentType.CONVERSABLE_AGENT: lambda params: autogen.ConversableAgent(
        system_message="You are a conversable AI assistant.", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.ASSISTANT_AGENT: lambda params: autogen.AssistantAgent(
        system_message="You are an AI assistant that helps with tasks.", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.USER_PROXY_AGENT: lambda params: autogen.UserProxyAgent(
        system_message="You are a proxy for the human user. You are the human admin",
        human_input_mode="ALWAYS",
        max_consecutive_auto_reply=10, 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.RETRIEVE_USER_PROXY_AGENT: lambda params: autogen.RetrieveUserProxyAgent(
        system_message="You are a proxy for the human user with retrieval capabilities.",
        human_input_mode="ALWAYS",
        max_consecutive_auto_reply=10,
        retrieve_config={"task": "qa", "docs_path": "path/to/your/documents"},
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.TEACHABLE_AGENT: lambda params: autogen.TeachableAgent(
        system_message="You are an AI assistant that can learn from interactions.",
        teach_config={"verbosity": 0, "reset_db": False, "path_to_db_dir": "./teachable_agent_db"},
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.AUTOGEN_CODER: lambda params: autogen.AutoGenCoder(
        system_message="You are an AI assistant specialized in coding tasks.", 
        code_execution_config=default_code_execution_config,
        llm_config = params["llm_config"]
    ),
    AgentType.CONFIGURABLE_AGENT: lambda params: autogen.ConfigurableAgent(
        system_message="You are a configurable AI assistant.",
        human_input_mode="NEVER", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.HUMAN_PROXY_AGENT: lambda params: autogen.HumanProxyAgent(
        system_message="You are a human proxy agent.",
        human_input_mode="ALWAYS",
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
        AgentType.GROUP_CHAT: lambda params: autogen.GroupChat(
        agents=params.get("agents", None),
        messages=params.get("messages", None),
        max_round=params.get("max_round", 12),
        speaker_selection_method=params.get("speaker_selection_method", "round_robin"),
        allow_repeat_speaker=params.get("allow_repeat_speaker", False), 
        # llm_config=params.get("llm_config", None)
    ),
    AgentType.GROUP_CHAT_MANAGER: lambda params: autogen.GroupChatManager(
        system_message = "You are managing a group chat.",
        groupchat=params.get("groupchat", None),
        name=params.get("name", "Group Chat Manager"),
        
        #max_consecutive_auto_reply = 10,
        #human_input_mode = "NEVER",
        # llm_config=params.get("llm_config", None)
        #max_round=params.get("max_round", 12),
    ),
}



File: .\config_manager.py
-------------------------
import json
import random

class ConfigManager:
    def __init__(self, config_file):
        self.config_file = config_file
        self.model_configs = self.load_model_configs()

    def load_model_configs(self):
        with open(self.config_file, 'r') as f:
            return json.load(f)

    def get_available_models(self):
        return list(self.model_configs.keys())

    def get_model_config(self, model_name):
        return self.model_configs.get(model_name)

    def get_random_model_config(self):
        available_models = self.get_available_models()
        if available_models:
            model_name = random.choice(available_models)
            return self.get_model_config(model_name)
        return None

    def update_model_config(self, model_name, config):
        self.model_configs[model_name] = config
        self.save_model_configs()

    def save_model_configs(self):
        with open(self.config_file, 'w') as f:
            json.dump(self.model_configs, f, indent=4)


File: .\connect.py
------------------
import autogen
import agents_ls
from agent_ls import AgentType


def conversation():
    agent_manager = agents_ls.AgentManagerLS()
    # model_name="internlm2_5-20b-chat-q3_k_m.gguf"
    steve_assistant = agent_manager.create(AgentType.ASSISTANT_AGENT, name="Steve")
    user_proxy = agent_manager.create(AgentType.USER_PROXY_AGENT, name="User")
    
    user_proxy.initiate_chat(steve_assistant, message="Tell me an interesting fact!")
    
    
def group_chat():
    
    agent_manager = agents_ls.AgentManagerLS()
    
    user_proxy = agent_manager.create(AgentType.USER_PROXY_AGENT, name="User_proxy")
    
    user_proxy.code_execution_config={"last_n_messages": 2, "work_dir": "coding", "use_docker": True}
    user_proxy.code_human_input_mode="TERMINATE"
    
    engineer = agent_manager.create(AgentType.ASSISTANT_AGENT, name="Engineer") 
    
    scientist = agent_manager.create(AgentType.ASSISTANT_AGENT, name="scientistAgentType")
    planner = agent_manager.create(AgentType.ASSISTANT_AGENT, name="plannerAgentType") 
    critic = agent_manager.create(AgentType.ASSISTANT_AGENT, name="criticAgentType") 
    
    agents = [user_proxy, engineer, scientist, planner, critic]
    group_chat = agent_manager.create(AgentType.GROUP_CHAT, agent_params={ "agents": agents})
    
    group_chat_manager = agent_manager.create(AgentType.GROUP_CHAT_MANAGER, agent_params={"groupchat": group_chat})
    
    user_proxy.initiate_chat(
        group_chat_manager,
        message="""
        Task for Investment Research Agents: Comprehensive Insights and Recommendations on AI and Generative AI Investments
Objective:
Identify investment opportunities in AI and generative AI sectors that offer a balance of safety and profitability, considering geopolitical risks, such as a potential conflict involving China and Taiwan, and its impact on global AI hardware and software markets.
Tasks:
1. Identify Key Sectors for Investment:
Industries Benefiting from AI and Generative AI:
Technology: Companies involved in AI software development, cloud computing, and data centers.
Semiconductors: Firms producing chips essential for AI and generative AI, particularly those related to GPU and TPU manufacturing.
Healthcare: AI-driven diagnostics, drug discovery, and telemedicine, which utilize generative AI for innovation.
Creative Industries: Sectors such as media, entertainment, and advertising, where generative AI is transforming content creation.
Sub-Sectors with High Growth Potential:
AI Cloud Services: Providers offering AI as a Service (AIaaS) and cloud-based generative AI solutions.
AI-Driven Healthcare: Companies using AI for personalized medicine, medical imaging, and generative AI in drug development.
Generative AI Applications: Firms focused on natural language processing, image generation, and automated content creation.
2. Evaluate Funds and ETFs:
Global and Technology-Focused Funds:
Identify funds with significant exposure to AI and generative AI, focusing on those with a global reach.
Evaluate funds that specifically target semiconductor companies, given their critical role in AI hardware.
Funds with Geopolitical Risk Mitigation:
Assess funds that diversify away from high-risk regions (e.g., Taiwan) and include companies with manufacturing and supply chain resilience.
Investigate funds that emphasize companies with diversified production capabilities across multiple regions.
3. Assess Geopolitical Risks:
Impact of a Potential China-Taiwan Conflict:
Semiconductor Supply Chain: Analyze the impact on global AI hardware supply if Taiwan's semiconductor industry (e.g., TSMC) is disrupted. Determine how this could affect the availability and cost of AI hardware globally.
Regional Diversification: Evaluate which countries or regions (e.g., the U.S., South Korea, Japan) could emerge as alternative hubs for AI hardware production.
Stock and Fund Impact: Analyze how a conflict might affect specific funds or ETFs that have high exposure to Taiwanese semiconductor companies.
Global AI Leadership Shifts:
Potential New Leaders: Identify regions or countries that could become new AI and generative AI leaders due to geopolitical shifts. Assess their impact on global markets and investment opportunities.
Supply Chain Realignment: Research how companies and funds are repositioning their supply chains to mitigate geopolitical risks, focusing on those that are less reliant on high-risk regions.
4. Analyze Economic Impact and Risks:
Global Labor Markets and AI Adoption:
Investigate how increased AI and generative AI adoption could affect global labor markets, especially in industries like manufacturing, healthcare, and creative sectors.
Assess the potential for economic inequality and job displacement, and how this might influence long-term investment stability.
Sector-Specific Risks:
Healthcare AI: Explore regulatory challenges and ethical concerns that could impact the adoption of AI in healthcare, and how this might affect fund performance.
Creative AI: Consider the risks of copyright and intellectual property issues related to generative AI and their potential financial implications.
5. Investigate Ethical and Regulatory Considerations:
Current and Future Regulations:
Analyze how current regulations in key markets (e.g., EU, U.S., China) affect AI and generative AI development and deployment.
Anticipate potential regulatory changes that could impact AI and generative AI sectors, particularly in areas like data privacy, ethical AI, and anti-monopoly laws.
Ethical Investment Opportunities:
Identify funds or ETFs that invest in companies committed to ethical AI practices, such as responsible AI development and ensuring fairness and transparency in AI algorithms.
Evaluate funds that support companies involved in AI and generative AI retraining programs for displaced workers, emphasizing long-term social sustainability.
6. Provide Comprehensive Investment Recommendations:
Specific Funds and ETFs:
Compile a list of recommended funds that balance safety and profitability in AI and generative AI sectors.
Focus on funds that offer global diversification, sector-specific exposure, and a mix of established and emerging market opportunities.
Consider funds that are positioned to withstand geopolitical risks, with investments in regions or companies with strong supply chain resilience.
Ranking of Investment Opportunities:
Rank funds based on potential return, risk exposure (including geopolitical and regulatory risks), and alignment with broader economic and technological trends.
Risk Mitigation Strategies:
Recommend strategies for managing potential downsides, including diversification across sectors and geographies, investing in funds with a defensive posture, and prioritizing long-term growth.
Deliverables:
Detailed Report: A comprehensive analysis covering each task, including data, charts, and growth projections.
Investment Recommendations List: Justification for each fund or ETF, focusing on safety, profitability, and risk management.
Risk Assessment Report: Strategies to manage potential downsides, including geopolitical risks and supply chain disruptions.

        """,
    )
    
    
def display_menu():
    print("\n--- Main Menu ---")
    print("1. Start a conversation")
    print("2. Join a group chat")
    print("3. Exit")
    print("----------------")

def get_user_choice():
    while True:
        choice = input("Enter your choice (1-3): ")
        if choice in ['1', '2', '3']:
            return int(choice)
        else:
            print("Invalid input. Please enter a number between 1 and 3.")

if __name__ == "__main__":
    while True:
        display_menu()
        user_choice = get_user_choice()
        
        if user_choice == 1:
            conversation()
        elif user_choice == 2:
            group_chat()
        elif user_choice == 3:
            print("Exiting the program. Goodbye!")
            break
        
        input("\nPress Enter to return to the main menu...")


File: .\directory_structure_and_contents.txt
--------------------------------------------

Directory: .
============

File: .\agents_ls.py
--------------------
import json
import random
import uuid

import agent_ls
import model_manager
from agent_ls import AgentType, AGENT_CREATORS, AGENTS_USING_NAMES

config_file_name = "model_library.json"


class AgentManagerLS:
    '''
    Use AgentType Enum to see the agent types available.
    If a model is not specified, a random (loaded) model will be choosen.
    '''
    def __init__(self):
        self.model_configs = self.load_model_configs(config_file_name)
        self.loaded_models = model_manager.load_and_save_models()
        self.matched_models = self.match_models_with_configs(self.loaded_models, self.model_configs)
        
        # Default code execution config
        self.default_code_execution_config = {"use_docker": False}
        
        
    @staticmethod
    def load_model_configs(filename):
        with open(filename, 'r') as f:
            return json.load(f)


    @staticmethod
    def match_models_with_configs(loaded_models, model_configs):
        matched_models = {}
        for model in loaded_models:
            short_name = model.rsplit('/', 1)[-1]
            if short_name in model_configs:
                matched_models[short_name] = model_configs[short_name]
        return matched_models


    def get_available_models(self) -> str:
        return list(self.matched_models.keys())
    
    
    def print_available_models(self):
        print("\nAvailable models:")
        for model_name in self.get_available_models():
            print(f"- {model_name}")


    def create(self, agent_type: AgentType, name = "", model_name = "", **kwargs):
        
        #print("agent_type", agent_type, "AGENT_CREATORS", AGENT_CREATORS)
        # group_chat = agent_manager.create("Chat Group", AgentType.CHAT_GROUP)
        if agent_type not in AGENT_CREATORS:
            
            raise ValueError(f"Unknown agent type: {agent_type}")
        
        if not model_name:
            available_models = self.get_available_models()
            
            if available_models:
                model_name = random.choice(available_models)
                print(f"Agent {name} {agent_type} was assigned a random model: {model_name}")
        
        else:
            print(f"Agent {name} {agent_type} was assigned the model: {model_name}")
                
        if model_name and model_name not in self.matched_models:
            self.print_available_models()
            raise ValueError(f"Model {model_name} not found in available models.")

        model_config = self.matched_models[model_name]
        config_list = model_config['config_list']
        llm_config = {
            "config_list": config_list,
            "cache_seed": model_config['cache_seed'],
            "temperature": config_list[0]['temperature'],
            "max_tokens": model_config['max_tokens']
        }
        
        
        
        if agent_type in AGENTS_USING_NAMES:
            if not name:
                name = f"{model_name}_{uuid.uuid4().hex[:8]}"
            kwargs.update({"name": name})
            
        kwargs.update({"llm_config": llm_config})
        
        # Ensure agents is provided for GroupChat
        if agent_type == AgentType.GROUP_CHAT:
            if "agents" not in kwargs or kwargs["agents"] is None:
                raise ValueError("GroupChat requires a non-empty list of agents in the 'agents' parameter.")

        return AGENT_CREATORS[agent_type](kwargs)


# Usage example
if __name__ == "__main__":
    agent_ls = AgentManagerLS()
    
    print("Available agent types:")
    for agent_type in AgentType:
        print(f"- {agent_type.name}")
    
    available_models = agent_ls.get_available_models()
    model = random.choice(available_models)
    
    if available_models:
        model = agent_ls.user_specified_model
        if not model:
            model = random.choice(available_models)
            
        conversable_agent = agent_ls.create(AgentType.CONVERSABLE_AGENT)
        
        # Create an assistant agent with the second available model (if there is one)
        if len(available_models) > 1:
            assistant_agent = agent_ls.create(AgentType.ASSISTANT_AGENT, available_models[1])
            print(f"Created an Assistant Agent: {assistant_agent.name}")
    else:
        print("No models available. Please check your model configurations.")



File: .\agent_factory.py
------------------------
import autogen
from enum import Enum
import uuid

class AgentType(Enum):
    CONVERSABLE_AGENT = "conversable_agent"
    ASSISTANT_AGENT = "assistant_agent"
    USER_PROXY_AGENT = "user_proxy_agent"
    RETRIEVE_USER_PROXY_AGENT = "retrieve_user_proxy_agent"
    GROUP_CHAT_MANAGER = "group_chat_manager"
    TEACHABLE_AGENT = "teachable_agent"
    AUTOGEN_CODER = "autogen_coder"
    CONFIGURABLE_AGENT = "configurable_agent"
    HUMAN_PROXY_AGENT = "human_proxy_agent"
    GROUP_CHAT = "chat_group"

AGENTS_USING_NAMES = [
    AgentType.CONVERSABLE_AGENT,
    AgentType.ASSISTANT_AGENT,
    AgentType.USER_PROXY_AGENT,
    AgentType.TEACHABLE_AGENT,
    AgentType.AUTOGEN_CODER,
    AgentType.CONFIGURABLE_AGENT,
    AgentType.HUMAN_PROXY_AGENT,
]

default_code_execution_config = {"use_docker": True}

AGENT_CREATORS = {
    AgentType.CONVERSABLE_AGENT: lambda params: autogen.ConversableAgent(**params),
    AgentType.ASSISTANT_AGENT: lambda params: autogen.AssistantAgent(**params),
    AgentType.USER_PROXY_AGENT: lambda params: autogen.UserProxyAgent(**params),
    AgentType.RETRIEVE_USER_PROXY_AGENT: lambda params: autogen.RetrieveUserProxyAgent(**params),
    AgentType.TEACHABLE_AGENT: lambda params: autogen.TeachableAgent(**params),
    AgentType.AUTOGEN_CODER: lambda params: autogen.AutoGenCoder(**params),
    AgentType.CONFIGURABLE_AGENT: lambda params: autogen.ConfigurableAgent(**params),
    AgentType.HUMAN_PROXY_AGENT: lambda params: autogen.HumanProxyAgent(**params),
    AgentType.GROUP_CHAT: lambda params: autogen.GroupChat(**params),
    AgentType.GROUP_CHAT_MANAGER: lambda params: autogen.GroupChatManager(**params),
}

def create_agent(agent_type, name="", model_config=None, obj=None, **kwargs):
    if agent_type not in AGENT_CREATORS:
        raise ValueError(f"Unknown agent type: {agent_type}")

    agent_params = {
        "name": name or f"Agent_{uuid.uuid4().hex[:8]}",
        "llm_config": model_config,
        "code_execution_config": default_code_execution_config,
    }
    
    if agent_type in AGENTS_USING_NAMES:
        agent_params["system_message"] = f"You are an AI assistant named {agent_params['name']}."
    
    agent_params.update(kwargs)
    
    if agent_type == AgentType.GROUP_CHAT:
        return AGENT_CREATORS[agent_type](obj)
    elif agent_type == AgentType.GROUP_CHAT_MANAGER:
        return AGENT_CREATORS[agent_type](groupchat=obj)
    else:
        return AGENT_CREATORS[agent_type](agent_params)


File: .\agent_ls.py
-------------------
from enum import Enum
import autogen

class AgentType(Enum):
    CONVERSABLE_AGENT = "conversable_agent"
    ASSISTANT_AGENT = "assistant_agent"
    USER_PROXY_AGENT = "user_proxy_agent"
    RETRIEVE_USER_PROXY_AGENT = "retrieve_user_proxy_agent"
    GROUP_CHAT_MANAGER = "group_chat_manager"
    TEACHABLE_AGENT = "teachable_agent"
    AUTOGEN_CODER = "autogen_coder"
    CONFIGURABLE_AGENT = "configurable_agent"
    HUMAN_PROXY_AGENT = "human_proxy_agent"
    GROUP_CHAT = "chat_group"
    
AGENTS_USING_NAMES = [
    AgentType.CONVERSABLE_AGENT,
    AgentType.ASSISTANT_AGENT,
    AgentType.USER_PROXY_AGENT,
    AgentType.TEACHABLE_AGENT,
    AgentType.AUTOGEN_CODER,
    AgentType.CONFIGURABLE_AGENT,
    AgentType.HUMAN_PROXY_AGENT,
]

default_code_execution_config = {"use_docker": True}
    
AGENT_CREATORS = {
    AgentType.CONVERSABLE_AGENT: lambda params: autogen.ConversableAgent(
        system_message="You are a conversable AI assistant.", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.ASSISTANT_AGENT: lambda params: autogen.AssistantAgent(
        system_message="You are an AI assistant that helps with tasks.", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.USER_PROXY_AGENT: lambda params: autogen.UserProxyAgent(
        system_message="You are a proxy for the human user. You are the human admin",
        human_input_mode="ALWAYS",
        max_consecutive_auto_reply=10, 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.RETRIEVE_USER_PROXY_AGENT: lambda params: autogen.RetrieveUserProxyAgent(
        system_message="You are a proxy for the human user with retrieval capabilities.",
        human_input_mode="ALWAYS",
        max_consecutive_auto_reply=10,
        retrieve_config={"task": "qa", "docs_path": "path/to/your/documents"},
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.TEACHABLE_AGENT: lambda params: autogen.TeachableAgent(
        system_message="You are an AI assistant that can learn from interactions.",
        teach_config={"verbosity": 0, "reset_db": False, "path_to_db_dir": "./teachable_agent_db"},
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.AUTOGEN_CODER: lambda params: autogen.AutoGenCoder(
        system_message="You are an AI assistant specialized in coding tasks.", 
        code_execution_config=default_code_execution_config,
        llm_config = params["llm_config"]
    ),
    AgentType.CONFIGURABLE_AGENT: lambda params: autogen.ConfigurableAgent(
        system_message="You are a configurable AI assistant.",
        human_input_mode="NEVER", 
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
    AgentType.HUMAN_PROXY_AGENT: lambda params: autogen.HumanProxyAgent(
        system_message="You are a human proxy agent.",
        human_input_mode="ALWAYS",
        code_execution_config=default_code_execution_config,
        name = params["name"],
        llm_config = params["llm_config"]
    ),
        AgentType.GROUP_CHAT: lambda params: autogen.GroupChat(
        agents=params.get("agents", None),
        messages=params.get("messages", None),
        max_round=params.get("max_round", 12),
        speaker_selection_method=params.get("speaker_selection_method", "round_robin"),
        allow_repeat_speaker=params.get("allow_repeat_speaker", False), 
        # llm_config=params.get("llm_config", None)
    ),
    AgentType.GROUP_CHAT_MANAGER: lambda params: autogen.GroupChatManager(
        system_message = "You are managing a group chat.",
        groupchat=params.get("groupchat", None),
        name=params.get("name", "Group Chat Manager"),
        
        #max_consecutive_auto_reply = 10,
        #human_input_mode = "NEVER",
        # llm_config=params.get("llm_config", None)
        #max_round=params.get("max_round", 12),
    ),
}



File: .\config_manager.py
-------------------------
import json
import random

class ConfigManager:
    def __init__(self, config_file):
        self.config_file = config_file
        self.model_configs = self.load_model_configs()

    def load_model_configs(self):
        with open(self.config_file, 'r') as f:
            return json.load(f)

    def get_available_models(self):
        return list(self.model_configs.keys())

    def get_model_config(self, model_name):
        return self.model_configs.get(model_name)

    def get_random_model_config(self):
        available_models = self.get_available_models()
        if available_models:
            model_name = random.choice(available_models)
            return self.get_model_config(model_name)
        return None

    def update_model_config(self, model_name, config):
        self.model_configs[model_name] = config
        self.save_model_configs()

    def save_model_configs(self):
        with open(self.config_file, 'w') as f:
            json.dump(self.model_configs, f, indent=4)


File: .\connect.py
------------------



File: .\lm_studio_manager new.py
--------------------------------
import requests
from requests.exceptions import RequestException, HTTPError

class LMStudioManager:
    def __init__(self, base_url="http://localhost:1234"):
        self.base_url = base_url

    def get_status_and_loaded_models(self):
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            response.raise_for_status()
            loaded_models = response.json()["data"]
            return {"status": "active", "models": [model["id"] for model in loaded_models]} if loaded_models else {"status": "idle", "models": []}
        except RequestException as e:
            return {"status": "error", "message": str(e)}

    def get_model_details(self, model_id):
        try:
            response = requests.get(f"{self.base_url}/v1/models/{model_id}")
            response.raise_for_status()
            return response.json()
        except RequestException as e:
            return {"error": str(e)}

    def send_prompt(self, prompt, model_name, max_tokens=1000):
        try:
            response = requests.post(
                f"{self.base_url}/v1/completions",
                json={"prompt": prompt, "model": model_name, "max_tokens": max_tokens}
            )
            response.raise_for_status()
            return response.json()
        except RequestException as e:
            return {"error": str(e)}

    def send_chat(self, messages, model_name, max_tokens=100):
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json={"messages": messages, "model": model_name, "max_tokens": max_tokens}
            )
            response.raise_for_status()
            return response.json()
        except RequestException as e:
            return {"error": str(e)}


File: .\lm_studio_manager.py
----------------------------
import requests
import json
from requests.exceptions import RequestException, HTTPError

class LMStudioManager:
    def __init__(self, base_url="http://localhost:1234"):
        self.base_url = base_url
        
    def is_lm_studio_active(self):
        return self.get_status_and_loaded_models()["status"] == "active"
        
    def is_model_loaded(self, model):
        get_the_technical_name = lambda model: model.split()[-1] if " " in model else model
        model = get_the_technical_name(model)
        
        models_statuses = self.get_status_and_loaded_models()["models"]
        
        model_in_list = any(model in m for m in models_statuses)

        print("Model Status Requested:", model)
        print("Model loaded", models_statuses)
        print("Model found" if model_in_list else "Model was not found!")
        return model_in_list
    
    def get_loaded_models(self):
        return self.get_status_and_loaded_models()["models"]
            
    def get_status_and_loaded_models(self):
        """Check the status of loaded models in LM Studio."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            response.raise_for_status()
            #print("Models loaded on the LM Studio server", response)
            loaded_models = response.json()["data"]
            # print("loaded models", loaded_models)
            if loaded_models:
                return {"status": "active", "models": [model["id"] for model in loaded_models]}
            else:
                return {"status": "idle", "models": []}
        except HTTPError as e:
            return {"status": "error", "message": f"HTTP error occurred: {e}"}
        except RequestException as e:
            return {"status": "error", "message": f"An error occurred: {e}"}

    '''
    
    Below are methods that are not currently in use.
    These methods extend the capabilities of LMStudioManager for future use.
    
    '''
        
    def list_available_models(self):
        """List all available models in LM Studio."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            response.raise_for_status()
            return response.json()["data"]
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}

    def send_prompt(self, prompt, model_name, max_tokens=1000):
        """Send a prompt to a specified model and get the response."""
        try:
            response = requests.post(
                f"{self.base_url}/v1/completions",
                json={
                    "prompt": prompt,
                    "model": model_name,
                    "max_tokens": max_tokens
                }
            )
            response.raise_for_status()
            return response.json()
        except HTTPError as e:
            return {"error": f"HTTP error occurred: {e}"}
        except RequestException as e:
            return {"error": f"An error occurred: {e}"}
        
    def send_chat(self, messages, model_name, max_tokens=100):
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json={
                    "messages": messages,
                    "model": model_name,
                    "max_tokens": max_tokens
                }
            )
            response.raise_for_status()
            return response.json()
        except RequestException as e:
            return {"error": f"An error occurred: {e}"}
        
    def generate_embedding(self, input_text, model_name):
        try:
            response = requests.post(
                f"{self.base_url}/v1/embeddings",
                json={
                    "input": input_text,
                    "model": model_name
                }
            )
            response.raise_for_status()
            return response.json()
        except RequestException as e:
            return {"error": f"An error occurred: {e}"}

    def get_model_details(self, model_id):
        """Get detailed information about a specific model."""
        try:
            response = requests.get(f"{self.base_url}/v1/models/{model_id}")
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}

    def save_config(self, config, filename="lm_studio_config.json"):
        """Save the current configuration to a JSON file."""
        with open(filename, 'w') as f:
            json.dump(config, f, indent=4)

    def load_config(self, filename="lm_studio_config.json"):
        """Load configuration from a JSON file."""
        try:
            with open(filename, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {"error": "Configuration file not found"}
        except json.JSONDecodeError:
            return {"error": "Invalid JSON in configuration file"}
        
    def _print_model_details(self, model_name=None):
        """
        Prints detailed information about loaded models.
        
        :param model_name: Optional. The name or ID of a specific model to print details for.
        :return: A dictionary with 'success' (boolean) and 'message' (string) keys
        """
        # Get the list of loaded models
        loaded_models = self.get_status_and_loaded_models()

        if not loaded_models:
            return {
                'success': False,
                'message': "No models are currently loaded."
            }

        if model_name:
            # If a specific model is requested, check if it's loaded
            if model_name not in loaded_models:
                return {
                    'success': False,
                    'message': f"Error: Model '{model_name}' is not currently loaded."
                }
            models_to_print = [model_name]
        else:
            # If no specific model is requested, print details for all loaded models
            models_to_print = loaded_models

        for model in models_to_print:
            # Get detailed information about the model
            model_info = self.get_model_details(model)

            if "error" in model_info:
                print(f"Error retrieving details for model '{model}': {model_info['error']}")
                continue

            # Print the details
            print(f"\nDetails for model: {model}")
            print("-" * 40)
            
            # Print each key-value pair in the model_info
            for key, value in model_info.items():
                print(f"{key.capitalize()}: {value}")

            print("-" * 40)

        return {
            'success': True,
            'message': "Model details printed successfully."
        }
        
    def _print_loaded_models(self, models):
        """
        Prints detailed information about all currently loaded models.
        """

        if models["status"] == "active" and (loaded_models := models["models"]):
            for model in loaded_models:
                # Get detailed information about the model
                print("Model:", model)
                model_details = self.get_model_details(model)

                print(f"\nDetails for model: {model}")
                print("-" * 40)

                if "error" in model_details:
                    print(f"Error retrieving model details: {model_details['error']}")
                else:
                    # Print each key-value pair in the model_info
                    for key, value in model_details.items():
                        print(f"{key.capitalize()}: {value}")

                print("-" * 40)
            
        else:
            print("LM Studio has no model loaded.") if loaded_models else print("LM Studio is not active.")


File: .\main.py
---------------
from config_manager import ConfigManager
from lm_studio_manager import LMStudioManager
from agent_factory import AgentType, create_agent

class AgentManager:
    def __init__(self, config_file):
        self.config_manager = ConfigManager(config_file)
        self.lm_studio = LMStudioManager()

    def create_agent(self, agent_type, name="", model_name="", obj=None, **kwargs):
        if agent_type in [AgentType.GROUP_CHAT, AgentType.GROUP_CHAT_MANAGER]:
            return create_agent(agent_type, name=name, obj=obj, **kwargs)

        if not model_name:
            model_config = self.config_manager.get_random_model_config()
        else:
            model_config = self.config_manager.get_model_config(model_name)

        if not model_config:
            raise ValueError("No valid model configuration found.")

        return create_agent(agent_type, name, model_config, **kwargs)

    def start_conversation(self):
        user_proxy = self.create_agent(AgentType.USER_PROXY_AGENT, name="User")
        assistant = self.create_agent(AgentType.ASSISTANT_AGENT, name="Assistant")
        user_proxy.initiate_chat(assistant, message="Tell me an interesting fact!")

    def start_group_chat(self):
        user_proxy = self.create_agent(AgentType.USER_PROXY_AGENT, name="User_proxy")
        engineer = self.create_agent(AgentType.ASSISTANT_AGENT, name="Engineer")
        scientist = self.create_agent(AgentType.ASSISTANT_AGENT, name="Scientist")
        planner = self.create_agent(AgentType.ASSISTANT_AGENT, name="Planner")
        critic = self.create_agent(AgentType.ASSISTANT_AGENT, name="Critic")

        group_chat = self.create_agent(AgentType.GROUP_CHAT, obj=[user_proxy, engineer, scientist, planner, critic])
        group_chat_manager = self.create_agent(AgentType.GROUP_CHAT_MANAGER, obj=group_chat)

        user_proxy.initiate_chat(group_chat_manager, message="Let's discuss the future of AI.")

def main():
    agent_manager = AgentManager("model_library.json")

    while True:
        print("\n--- Main Menu ---")
        print("1. Start a conversation")
        print("2. Join a group chat")
        print("3. Exit")
        choice = input("Enter your choice (1-3): ")

        if choice == '1':
            agent_manager.start_conversation()
        elif choice == '2':
            agent_manager.start_group_chat()
        elif choice == '3':
            print("Exiting the program. Goodbye!")
            break
        else:
            print("Invalid input. Please enter a number between 1 and 3.")

        input("\nPress Enter to return to the main menu...")

if __name__ == "__main__":
    main()


File: .\model_library.json
--------------------------
{
    "phi_2_gguf": {
        "config_list": [
            {
                "model": "TheBloke/phi-2-GGUF",
                "base_url": "base_url",
                "api_key": "api_key",
                "temperature": 0.5
            }
        ],
        "cache_seed": "None",
        "max_tokens": 4096
    },
    "phi_3_mini_4k_instruct_q4": {
        "config_list": [
            {
                "model": "microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-q4.gguf",
                "base_url": "base_url",
                "api_key": "api_key",
                "temperature": 0.5
            }
        ],
        "cache_seed": "None",
        "max_tokens": 4096
    },
    "llama_3_1_8B_instruct_Q8_0": {
        "config_list": [
            {
                "model": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
                "base_url": "base_url",
                "api_key": "api_key",
                "temperature": 0.7
            }
        ],
        "cache_seed": "None",
        "max_tokens": 16000
    },
    "internlm2_5_20b_chat_q3_k_m": {
        "config_list": [
            {
                "model": "internlm/internlm2_5-20b-chat-gguf/internlm2_5-20b-chat-q3_k_m.gguf",
                "base_url": "base_url",
                "api_key": "api_key",
                "temperature": 0.7
            }
        ],
        "cache_seed": "None",
        "max_tokens": 12000
    },
    "Meta-Llama-3.1-8B-Instruct-Q8_0.gguf": {
        "config_list": [
            {
                "model": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf",
                "base_url": "http://localhost:1234/v1",
                "api_key": "lm-studio",
                "temperature": 0.5
            }
        ],
        "cache_seed": null,
        "max_tokens": 1024
    },
    "internlm2_5-20b-chat-q3_k_m.gguf": {
        "config_list": [
            {
                "model": "internlm/internlm2_5-20b-chat-gguf/internlm2_5-20b-chat-q3_k_m.gguf",
                "base_url": "http://localhost:1234/v1",
                "api_key": "lm-studio",
                "temperature": 0.5
            }
        ],
        "cache_seed": null,
        "max_tokens": 1024
    },
    "phi-3-mini-4k-instruct-q4": {
        "config_list": [
            {
                "model": "phi-3-mini-4k-instruct-q4",
                "base_url": "http://localhost:1234/v1",
                "api_key": "lm-studio",
                "temperature": 0.7
            }
        ],
        "cache_seed": null,
        "max_tokens": 1024
    },
    "meta-llama-3.1-8b-instruct-q8_0": {
        "config_list": [
            {
                "model": "meta-llama-3.1-8b-instruct-q8_0",
                "base_url": "http://localhost:1234/v1",
                "api_key": "lm-studio",
                "temperature": 0.7
            }
        ],
        "cache_seed": null,
        "max_tokens": 1024
    },
    "internlm2_5-20b-chat-q3_k_m": {
        "config_list": [
            {
                "model": "internlm2_5-20b-chat-q3_k_m",
                "base_url": "http://localhost:1234/v1",
                "api_key": "lm-studio",
                "temperature": 0.7
            }
        ],
        "cache_seed": null,
        "max_tokens": 1024
    }
}


File: .\model_manager.py
------------------------
import json
import os
import lm_studio_manager

base_url = "http://localhost:1234/v1"
api_key = "lm-studio"
lsm = lm_studio_manager.LMStudioManager()
config_file_name = "model_library.json"

def get_short_name(model):
    return model.rsplit('/', 1)[-1]

def create_model_config(model_name, max_tokens=1024, temperature=0.7):
    return {
        f"{get_short_name(model_name)}": {
            "config_list": [
                {
                    "model": model_name,
                    "base_url": base_url,
                    "api_key": api_key,
                    "temperature": temperature
                },
            ],
            "cache_seed": None,
            "max_tokens": max_tokens
        }
    }
    
# "status": "active", "models"
def load_and_save_models():
    models_and_status = lsm.get_status_and_loaded_models()
    
    if models_and_status["status"] == "error":
        print("LM Studio Error", models_and_status["message"])
        return []
    
    models = models_and_status["models"]
    
    if not models:
        print("LM Studio is not active")
        return []
    
    print("Models", models)
    
    # Convert the list of models to a dictionary
    model_dict = [create_model_config(model) for model in models]
    save_model_library(config_file_name, model_dict)
    
    return models 
    
    
def load_model_library(filename):
    with open(filename, 'r') as f:
        return json.load(f)

def save_model_library(filename, model_configs: list):
    # Load the existing data from the file
    existing_data = load_model_library(filename)

    # Add only new models, without overwriting existing ones
    for config in model_configs:
        for key, value in config.items():
            if key not in existing_data:
                existing_data[key] = value

    # Write the updated data back to the file
    with open(filename, 'w') as f:
        json.dump(existing_data, f, indent=4)


def add_model_to_library(model_library, name, model_config):
    if name in model_library:
        print(f"Model {name} already exists. Skipping addition.")
    else:
        model_library[name] = model_config


def load_models_from_json(filename):
    loaded_models = load_model_library(filename)
    model_library = {}

    for model_name, model_data in loaded_models.items():
        config_list = model_data["config_list"]
        if config_list:
            model_config = config_list[0]
            model_library[model_name] = create_model_config(
                model_name=model_config["model"],
                max_tokens=model_data["max_tokens"],
                temperature=model_config["temperature"]
            )
    
    return model_library


File: .\settings.json
---------------------



File: .\structure_print.py
--------------------------
import os

# Define the directories to exclude and the file extensions to include
excluded_dirs = {'.venv', 'custom,' '.\custom', 'old', 'structure_print', 'venv', '__pycache__', 'test_v2', 'test_v3',  'CO2EmissionForecasting', 'C02TransportCalculator', '.git', 'git'}
included_extensions = {'.py', '.txt', '.html', '.js', '.json', '.yml', '.yaml'}

# The name of the output file
output_filename = 'directory_structure_and_contents.txt'

# Set to track processed files
processed_files = set()

def is_included_file(file_name):
    # Check if the file has one of the allowed extensions
    return any(file_name.endswith(ext) for ext in included_extensions)

def write_file_content(file_path, output_file):
    # Write the name and content of the file to the output file
    output_file.write(f"\nFile: {file_path}\n")
    output_file.write(f"{'-'*len(f'File: {file_path}')}\n")
    with open(file_path, 'r', encoding='utf-8') as f:
        output_file.write(f.read() + '\n\n')

def traverse_directory(directory, output_file):
    # Traverse the directory structure
    for root, dirs, files in os.walk(directory):
        # Skip the excluded directories
        dirs[:] = [d for d in dirs if d not in excluded_dirs]

        # Write the directory path as a heading
        output_file.write(f"\nDirectory: {root}\n")
        output_file.write(f"{'='*len(f'Directory: {root}')}\n")
        
        for file in files:
            if is_included_file(file):
                file_path = os.path.join(root, file)
                # Check if the file has already been processed
                if file_path not in processed_files:
                    write_file_content(file_path, output_file)
                    # Mark the file as processed
                    processed_files.add(file_path)

# Run the script
with open(output_filename, 'w', encoding='utf-8') as output_file:
    traverse_directory('.', output_file)

print(f"Directory structure and contents have been saved to '{output_filename}'.")


